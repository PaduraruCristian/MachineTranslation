{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7367, 3584)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "langs = ['deu', 'esp', 'eng']\n",
    "all_embs = []\n",
    "for lang in langs:\n",
    "    train_embs = np.load(f'./embeddings/qwen/train_{lang}.npy')\n",
    "    all_embs.append(train_embs)\n",
    "all_embs = np.concatenate(all_embs, axis=0)\n",
    "np.save('./embeddings/qwen/train_deu-esp-eng.npy', all_embs)\n",
    "all_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import datasets_local\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "EMB_DIM = {\n",
    "    'lealla-large': 256,\n",
    "    'qwen2.5-7b': 3584,\n",
    "}\n",
    "SEEDS = [1007, 1013, 1019]\n",
    "train_kwargs = {\n",
    "    'output_dim': 1,\n",
    "    'optimizer': 'adamw',\n",
    "    'loss_fn': 'bce_wll',\n",
    "    'pred_fn': 'bce',\n",
    "    'lr': 1e-3,\n",
    "    'lr_min': 1e-5,\n",
    "    'n_epochs': 50,\n",
    "    'wd': 1e-3,\n",
    "    'bs': 512,\n",
    "}\n",
    "model_name = \"unsloth/QWEN2.5-7B\"\n",
    "model_path_name = model_name.lower().split('/')[-1]\n",
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_LANGS = ['eng', 'deu', 'esp', 'deu-esp-eng']\n",
    "OOD_LANGS = ['ron', 'ukr', 'hin']\n",
    "\n",
    "lang_mtds = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "lang_targets = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "lang_embeddings = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "for lang in OOD_LANGS:\n",
    "    mtd = pd.read_csv(f'./data/track_c/dev/{lang}.csv')\n",
    "    lang_mtds[lang] = mtd\n",
    "    mtd.drop(columns=['text'], inplace=True)\n",
    "\n",
    "    targets = [c for c in mtd.columns if c not in ['id', 'text']]\n",
    "    lang_targets[lang] = targets\n",
    "\n",
    "    embeddings = np.load(f'./embeddings/qwen/dev_c_{lang}.npy').astype(np.float32) ## QWEN embs are in fp16\n",
    "    embeddings = torch.tensor(embeddings).to(device)\n",
    "    embeddings /= embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "    lang_embeddings[lang] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9517/3526347638.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_net_st = torch.load(st_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F1 id 0.46012590297532263\n",
      "[0.208, 0.7060810810810811, 0.4205607476635514, 0.462882096069869, 0.5031055900621118]\n",
      "deu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9517/3526347638.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_net_st = torch.load(st_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F1 id 0.40440231731396586\n",
      "[0.6044776119402986, 0.5403508771929825, 0.20833333333333334, 0.44976076555023925, 0.4219409282700422, 0.20155038759689922]\n",
      "esp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9517/3526347638.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_net_st = torch.load(st_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F1 id 0.555840975566009\n",
      "[0.580246913580247, 0.6979166666666667, 0.4827586206896552, 0.6305418719211824, 0.47619047619047616, 0.46739130434782605]\n",
      "deu-esp-eng\n",
      "MACRO F1 id 0.5203235635616007\n",
      "[0.5271317829457364, 0.5776031434184675, 0.6437054631828978, 0.47965116279069775, 0.4432432432432432, 0.4506065857885615]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9517/3526347638.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_net_st = torch.load(st_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "for lang in ID_LANGS: # train lp on this lang only, make predictions\n",
    "    print(lang)\n",
    "\n",
    "    path = f'./classifiers/{model_path_name}/{lang}/'\n",
    "    output_path = f'./results/{model_path_name}/{lang}/'\n",
    "    os.makedirs(output_path, exist_ok=True) \n",
    "    predictions_path_c = f'./predictions/lp/qwen/{lang}/track_c/'\n",
    "    os.makedirs(predictions_path_c, exist_ok=True)\n",
    "    predictions_path_a = f'./predictions/lp/qwen/{lang}/track_a/'\n",
    "    os.makedirs(predictions_path_a, exist_ok=True)\n",
    "\n",
    "    emb_dim = EMB_DIM[model_path_name]\n",
    "\n",
    "    mtd = pd.read_csv(f'./data/track_a/train/{lang}.csv')\n",
    "    src_targets = [c for c in mtd.columns if c not in ['id', 'text']]\n",
    "    target_labels = {\n",
    "        c: mtd[c].to_numpy() for c in src_targets\n",
    "    }\n",
    "\n",
    "    ## mask targets for each lang\n",
    "    for ood_lang in OOD_LANGS:\n",
    "        for c in lang_targets[ood_lang]:\n",
    "            if c not in src_targets: # can't make predictions for this emotion; place zeros\n",
    "                lang_mtds[ood_lang][c] = [0 for _ in range(len(lang_mtds[ood_lang]))]\n",
    "    \n",
    "\n",
    "    train_indices, val_indices = utils.load_split_indices('lealla-large', lang)\n",
    "    train_labels = {\n",
    "        c: target_labels[c][train_indices] for c in src_targets\n",
    "    }\n",
    "    val_labels = {\n",
    "        c: target_labels[c][val_indices] for c in src_targets\n",
    "    }\n",
    "    \n",
    "    # label -1 means there is no label for that sample for emotion c\n",
    "    train_masks = {\n",
    "        c: train_labels[c] > -0.5 for c in train_labels.keys()\n",
    "    }\n",
    "    val_masks = {\n",
    "        c: val_labels[c] > -0.5 for c in val_labels.keys()\n",
    "    }\n",
    "\n",
    "    train_datasets = {\n",
    "        c: datasets_local.EmbeddingsDataset(None, train_labels[c][train_masks[c]]) for c in src_targets \n",
    "    }\n",
    "    val_datasets = {\n",
    "        c: datasets_local.EmbeddingsDataset(None, val_labels[c][val_masks[c]]) for c in src_targets\n",
    "    }\n",
    "\n",
    "\n",
    "    macro_f1s = []\n",
    "    path_layer = path + 'emb/'\n",
    "    os.makedirs(path_layer, exist_ok=True)\n",
    "    \n",
    "    embeddings = np.load(f'./embeddings/qwen/train_{lang}.npy').astype(np.float32) ## QWEN embs are in fp16\n",
    "    train_embeddings = torch.tensor(embeddings[train_indices]).to(device)\n",
    "    train_embeddings /= train_embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "    val_embeddings = torch.tensor(embeddings[val_indices]).to(device)\n",
    "    val_embeddings /= val_embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "    if os.path.exists(f'./embeddings/qwen/dev_a_{lang}.npy'):\n",
    "        dev_embeddings = np.load(f'./embeddings/qwen/dev_a_{lang}.npy').astype(np.float32)\n",
    "        dev_embeddings = torch.tensor(dev_embeddings).to(device)\n",
    "        dev_embeddings /= dev_embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "        same_lang_mtd = pd.read_csv(f'./data/track_a/dev/{lang}.csv')\n",
    "        same_lang_mtd.drop(columns=['text'], inplace=True)\n",
    "    else:\n",
    "        dev_embeddings = None\n",
    "        same_lang_mtd = None\n",
    "    same_lang_f1s = []\n",
    "    for c in src_targets:\n",
    "        st_path = path_layer + f'model_{c}.pt'\n",
    "        if os.path.exists(st_path):# and False:\n",
    "            best_net_st = torch.load(st_path, map_location='cpu')\n",
    "            f1s = best_net_st['f1s']\n",
    "            best_val_f1 = best_net_st['f1'] ## := max(f1s)\n",
    "        else:\n",
    "            train_datasets[c].embeddings = train_embeddings[train_masks[c]]\n",
    "            val_datasets[c].embeddings = val_embeddings[val_masks[c]]\n",
    "            f1s = []\n",
    "            best_f1 = 0\n",
    "            best_net_st = None\n",
    "            for seed in SEEDS:\n",
    "                net_st, best_val_f1 = utils.train_lp_balanced_class_loss(device, train_datasets[c], val_datasets[c], train_kwargs, seed, use_tqdm=False)\n",
    "                f1s.append(best_val_f1)\n",
    "                if best_val_f1 > best_f1:\n",
    "                    best_f1 = best_val_f1\n",
    "                    best_net_st = net_st\n",
    "                    best_net_st['f1'] = best_f1\n",
    "            \n",
    "            best_net_st['f1s'] = f1s \n",
    "            torch.save(best_net_st, st_path) # best classifier for this emotion\n",
    "        same_lang_f1s.append(best_val_f1)\n",
    "\n",
    "        ## make predictions on the OOD langs and save them to a folder\n",
    "        weight, bias = best_net_st['weight'].to(device), best_net_st['bias'].to(device)\n",
    "        for ood_lang in OOD_LANGS:\n",
    "            if c in lang_targets[ood_lang]:\n",
    "                scores_ = lang_embeddings[ood_lang] @ weight.T + bias\n",
    "                predictions_ = utils.get_predictions_bce(scores_).cpu().numpy()\n",
    "                lang_mtds[ood_lang][c] = predictions_\n",
    "        ## id predictions for track a\n",
    "        if dev_embeddings is not None:\n",
    "            scores_ = dev_embeddings @ weight.T + bias\n",
    "            predictions_ = utils.get_predictions_bce(scores_).cpu().numpy()\n",
    "            same_lang_mtd[c] = predictions_\n",
    "    # save mtd with the predictions\n",
    "    for ood_lang in OOD_LANGS:\n",
    "        lang_mtds[ood_lang].to_csv(predictions_path_c + f'pred_{ood_lang}.csv', index=False)\n",
    "    \n",
    "    if dev_embeddings is not None:\n",
    "        same_lang_mtd.to_csv(predictions_path_a + f'pred_{lang}.csv', index=False)\n",
    "\n",
    "    print('MACRO F1 id', np.mean(same_lang_f1s))\n",
    "    print(same_lang_f1s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# LogisticRegression#\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import datasets_local\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "EMB_DIM = {\n",
    "    'lealla-large': 256,\n",
    "    'qwen2.5-7b': 3584,\n",
    "}\n",
    "SEEDS = [1007, 1013, 1019]\n",
    "model_name = \"unsloth/QWEN2.5-7B\"\n",
    "model_path_name = model_name.lower().split('/')[-1]\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_LANGS = ['eng', 'deu', 'esp', 'deu-esp-eng']\n",
    "OOD_LANGS = ['ron', 'ukr', 'hin']\n",
    "\n",
    "lang_mtds = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "lang_targets = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "lang_embeddings = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "for lang in OOD_LANGS:\n",
    "    mtd = pd.read_csv(f'./data/track_c/dev/{lang}.csv')\n",
    "    lang_mtds[lang] = mtd\n",
    "    mtd.drop(columns=['text'], inplace=True)\n",
    "\n",
    "    targets = [c for c in mtd.columns if c not in ['id', 'text']]\n",
    "    lang_targets[lang] = targets\n",
    "\n",
    "    embeddings = np.load(f'./embeddings/qwen/dev_c_{lang}.npy').astype(np.float32) ## QWEN embs are in fp16\n",
    "    lang_embeddings[lang] = embeddings / np.sqrt(np.sum(np.square(embeddings), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F1 id 0.41049064950118846\n",
      "[0.21097046413502107, 0.6390041493775934, 0.3169014084507042, 0.4083044982698962, 0.4772727272727273]\n",
      "deu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F1 id 0.3785265567171258\n",
      "[0.6131386861313869, 0.5019011406844105, 0.2033898305084746, 0.430379746835443, 0.40740740740740744, 0.11494252873563218]\n",
      "esp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F1 id 0.5266006293289073\n",
      "[0.5508982035928144, 0.69, 0.4193548387096774, 0.5784313725490196, 0.47058823529411764, 0.45033112582781454]\n",
      "deu-esp-eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F1 id 0.5080279151143651\n",
      "[0.5252854812398042, 0.5621181262729125, 0.6357615894039734, 0.43879173290937995, 0.43115438108484005, 0.4550561797752809]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/py311/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for lang in ID_LANGS:\n",
    "    print(lang)\n",
    "\n",
    "    predictions_path_c = f'./predictions/lreg/qwen/{lang}/track_c/'\n",
    "    os.makedirs(predictions_path_c, exist_ok=True)\n",
    "    predictions_path_a = f'./predictions/lreg/qwen/{lang}/track_a/'\n",
    "    os.makedirs(predictions_path_a, exist_ok=True)\n",
    "\n",
    "    emb_dim = EMB_DIM[model_path_name]\n",
    "\n",
    "    mtd = pd.read_csv(f'./data/track_a/train/{lang}.csv')\n",
    "    src_targets = [c for c in mtd.columns if c not in ['id', 'text']]\n",
    "    target_labels = {\n",
    "        c: mtd[c].to_numpy() for c in src_targets\n",
    "    }\n",
    "\n",
    "    ## mask targets for each lang\n",
    "    for ood_lang in OOD_LANGS:\n",
    "        for c in lang_targets[ood_lang]:\n",
    "            if c not in src_targets: # can't make predictions for this emotion; place zeros\n",
    "                lang_mtds[ood_lang][c] = [0 for _ in range(len(lang_mtds[ood_lang]))]\n",
    "    \n",
    "\n",
    "    train_indices, val_indices = utils.load_split_indices('lealla-large', lang)\n",
    "    train_labels = {\n",
    "        c: target_labels[c][train_indices] for c in src_targets\n",
    "    }\n",
    "    val_labels = {\n",
    "        c: target_labels[c][val_indices] for c in src_targets\n",
    "    }\n",
    "    \n",
    "    # label -1 means there is no label for that sample for emotion c\n",
    "    train_masks = {\n",
    "        c: train_labels[c] > -0.5 for c in train_labels.keys()\n",
    "    }\n",
    "    val_masks = {\n",
    "        c: val_labels[c] > -0.5 for c in val_labels.keys()\n",
    "    }\n",
    "\n",
    "    macro_f1s = []\n",
    "\n",
    "    embeddings = np.load(f'./embeddings/qwen/train_{lang}.npy').astype(np.float32) ## QWEN embs are in fp16\n",
    "    train_embeddings = embeddings[train_indices]\n",
    "    train_embeddings /= np.sqrt(np.sum(np.square(train_embeddings), axis=1, keepdims=True))\n",
    "    val_embeddings = embeddings[val_indices]\n",
    "    val_embeddings /= np.sqrt(np.sum(np.square(val_embeddings), axis=1, keepdims=True))\n",
    "    if os.path.exists(f'./embeddings/qwen/dev_a_{lang}.npy'):\n",
    "        dev_embeddings = np.load(f'./embeddings/qwen/dev_a_{lang}.npy').astype(np.float32)\n",
    "        dev_embeddings /= np.sqrt(np.sum(np.square(dev_embeddings), axis=1, keepdims=True))\n",
    "        same_lang_mtd = pd.read_csv(f'./data/track_a/dev/{lang}.csv')\n",
    "        same_lang_mtd.drop(columns=['text'], inplace=True)\n",
    "    else:\n",
    "        dev_embeddings = None\n",
    "        same_lang_mtd = None\n",
    "    same_lang_f1s = []\n",
    "    for c in src_targets:\n",
    "        f1s = []\n",
    "        best_f1 = 0\n",
    "        best_lreg = None\n",
    "        train_c_embeddings = train_embeddings[train_masks[c]]\n",
    "        train_c_labels = train_labels[c][train_masks[c]]\n",
    "        val_c_embeddings = val_embeddings[val_masks[c]]\n",
    "        val_c_labels = val_labels[c][val_masks[c]]\n",
    "\n",
    "        for C in [1e-2, 1e-1, 1, 1e1, 1e2]:\n",
    "            # lreg = LogisticRegression(dual=False, C=C, random_state=SEEDS[0], class_weight='balanced', max_iter=100)\n",
    "            lreg = LogisticRegression(dual=True, C=C, random_state=SEEDS[0], class_weight='balanced', max_iter=100, solver='liblinear')\n",
    "            lreg.fit(train_c_embeddings, train_c_labels)\n",
    "            val_predictions = lreg.predict(val_c_embeddings)\n",
    "            f1 = f1_score(val_c_labels, val_predictions)\n",
    "            if f1 >= best_f1:\n",
    "                best_f1 = f1\n",
    "                best_lreg = lreg\n",
    "        \n",
    "        same_lang_f1s.append(best_f1)\n",
    "\n",
    "        ## make predictions on the OOD langs and save them to a folder\n",
    "        for ood_lang in OOD_LANGS:\n",
    "            if c in lang_targets[ood_lang]:\n",
    "                predictions_ = best_lreg.predict(lang_embeddings[ood_lang])\n",
    "                lang_mtds[ood_lang][c] = predictions_\n",
    "        if dev_embeddings is not None:\n",
    "            predictions_ = best_lreg.predict(dev_embeddings)\n",
    "            same_lang_mtd[c] = predictions_\n",
    "    # save mtd with the predictions\n",
    "    for ood_lang in OOD_LANGS:\n",
    "        lang_mtds[ood_lang].to_csv(predictions_path_c + f'pred_{ood_lang}.csv', index=False)\n",
    "    \n",
    "    if dev_embeddings is not None:\n",
    "        same_lang_mtd.to_csv(predictions_path_a + f'pred_{lang}.csv', index=False)\n",
    "\n",
    "    print('MACRO F1 id', np.mean(same_lang_f1s))\n",
    "    print(same_lang_f1s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "###### PCA\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n",
      "MACRO F1 id 0.3540222874585907\n",
      "[0.14225941422594143, 0.5217391304347826, 0.3286713286713286, 0.3630769230769231, 0.4143646408839779]\n",
      "deu\n",
      "MACRO F1 id 0.3161733738450248\n",
      "[0.4697508896797153, 0.47213114754098356, 0.1935483870967742, 0.28461538461538466, 0.3545454545454545, 0.12244897959183672]\n",
      "esp\n",
      "MACRO F1 id 0.3475748573740769\n",
      "[0.4158415841584158, 0.4524886877828054, 0.23030303030303031, 0.43859649122807015, 0.311377245508982, 0.23684210526315788]\n",
      "deu-esp-eng\n",
      "MACRO F1 id 0.44056359344184365\n",
      "[0.4178343949044586, 0.45161290322580644, 0.5469613259668508, 0.3795811518324608, 0.4087193460490463, 0.43867243867243866]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import datasets_local\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def normalize_embeddings(x):\n",
    "    return x / np.sqrt(np.sum(np.square(x), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "EMB_DIM = {\n",
    "    'lealla-large': 256,\n",
    "    'qwen2.5-7b': 3584,\n",
    "}\n",
    "\n",
    "PCA_DIMS = 64\n",
    "\n",
    "SEEDS = [1007, 1013, 1019]\n",
    "train_kwargs = {\n",
    "    'output_dim': 1,\n",
    "    'optimizer': 'adamw',\n",
    "    'loss_fn': 'bce_wll',\n",
    "    'pred_fn': 'bce',\n",
    "    'lr': 1e-3,\n",
    "    'lr_min': 1e-5,\n",
    "    'n_epochs': 50,\n",
    "    'wd': 1e-3,\n",
    "    'bs': 512,\n",
    "}\n",
    "model_name = \"unsloth/QWEN2.5-7B\"\n",
    "model_path_name = model_name.lower().split('/')[-1]\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "ID_LANGS = ['eng', 'deu', 'esp', 'deu-esp-eng']\n",
    "OOD_LANGS = ['ron', 'ukr', 'hin']\n",
    "\n",
    "lang_mtds = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "lang_targets = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "lang_embeddings = {\n",
    "    lang: None for lang in OOD_LANGS\n",
    "}\n",
    "for lang in OOD_LANGS:\n",
    "    mtd = pd.read_csv(f'./data/track_c/dev/{lang}.csv')\n",
    "    lang_mtds[lang] = mtd\n",
    "    mtd.drop(columns=['text'], inplace=True)\n",
    "\n",
    "    targets = [c for c in mtd.columns if c not in ['id', 'text']]\n",
    "    lang_targets[lang] = targets\n",
    "\n",
    "    embeddings = np.load(f'./embeddings/qwen/dev_c_{lang}.npy').astype(np.float32) ## QWEN embs are in fp16\n",
    "    embeddings = normalize_embeddings(embeddings)\n",
    "    lang_embeddings[lang] = embeddings\n",
    "\n",
    "for lang in ID_LANGS: # train lp on this lang only, make predictions\n",
    "    print(lang)\n",
    "\n",
    "    path = f'./classifiers/{model_path_name}/{lang}_pca/'\n",
    "    output_path = f'./results/{model_path_name}/{lang}_pca/'\n",
    "    os.makedirs(output_path, exist_ok=True) \n",
    "    predictions_path_c = f'./predictions/lp_pca/qwen/{lang}/track_c/'\n",
    "    os.makedirs(predictions_path_c, exist_ok=True)\n",
    "    predictions_path_a = f'./predictions/lp_pca/qwen/{lang}/track_a/'\n",
    "    os.makedirs(predictions_path_a, exist_ok=True)\n",
    "\n",
    "    mtd = pd.read_csv(f'./data/track_a/train/{lang}.csv')\n",
    "    src_targets = [c for c in mtd.columns if c not in ['id', 'text']]\n",
    "    target_labels = {\n",
    "        c: mtd[c].to_numpy() for c in src_targets\n",
    "    }\n",
    "\n",
    "    ## mask targets for each lang\n",
    "    for ood_lang in OOD_LANGS:\n",
    "        for c in lang_targets[ood_lang]:\n",
    "            if c not in src_targets: # can't make predictions for this emotion; place zeros\n",
    "                lang_mtds[ood_lang][c] = [0 for _ in range(len(lang_mtds[ood_lang]))]\n",
    "    \n",
    "\n",
    "    train_indices, val_indices = utils.load_split_indices('lealla-large', lang)\n",
    "    train_labels = {\n",
    "        c: target_labels[c][train_indices] for c in src_targets\n",
    "    }\n",
    "    val_labels = {\n",
    "        c: target_labels[c][val_indices] for c in src_targets\n",
    "    }\n",
    "    \n",
    "    # label -1 means there is no label for that sample for emotion c\n",
    "    train_masks = {\n",
    "        c: train_labels[c] > -0.5 for c in train_labels.keys()\n",
    "    }\n",
    "    val_masks = {\n",
    "        c: val_labels[c] > -0.5 for c in val_labels.keys()\n",
    "    }\n",
    "\n",
    "    train_datasets = {\n",
    "        c: datasets_local.EmbeddingsDataset(None, train_labels[c][train_masks[c]]) for c in src_targets \n",
    "    }\n",
    "    val_datasets = {\n",
    "        c: datasets_local.EmbeddingsDataset(None, val_labels[c][val_masks[c]]) for c in src_targets\n",
    "    }\n",
    "\n",
    "\n",
    "    macro_f1s = []\n",
    "    path_layer = path + 'emb/'\n",
    "    os.makedirs(path_layer, exist_ok=True)\n",
    "    \n",
    "    embeddings = np.load(f'./embeddings/qwen/train_{lang}.npy').astype(np.float32) ## QWEN embs are in fp16\n",
    "    embeddings = normalize_embeddings(embeddings)\n",
    "\n",
    "    pca = PCA(PCA_DIMS, random_state=1007)\n",
    "    embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    train_embeddings = torch.tensor(embeddings[train_indices]).to(device)\n",
    "    val_embeddings = torch.tensor(embeddings[val_indices]).to(device)\n",
    "    if os.path.exists(f'./embeddings/qwen/dev_a_{lang}.npy'):\n",
    "        dev_embeddings = np.load(f'./embeddings/qwen/dev_a_{lang}.npy').astype(np.float32)\n",
    "        dev_embeddings = normalize_embeddings(dev_embeddings)\n",
    "        dev_embeddings = pca.transform(dev_embeddings)\n",
    "        dev_embeddings = torch.tensor(dev_embeddings).to(device)\n",
    "\n",
    "        same_lang_mtd = pd.read_csv(f'./data/track_a/dev/{lang}.csv')\n",
    "        same_lang_mtd.drop(columns=['text'], inplace=True)\n",
    "    else:\n",
    "        dev_embeddings = None\n",
    "        same_lang_mtd = None\n",
    "    same_lang_f1s = []\n",
    "    for c in src_targets:\n",
    "        st_path = path_layer + f'model_{c}.pt'\n",
    "        if os.path.exists(st_path) and False:\n",
    "            best_net_st = torch.load(st_path, map_location='cpu')\n",
    "            f1s = best_net_st['f1s']\n",
    "            best_val_f1 = best_net_st['f1'] ## := max(f1s)\n",
    "        else:\n",
    "            train_datasets[c].embeddings = train_embeddings[train_masks[c]]\n",
    "            val_datasets[c].embeddings = val_embeddings[val_masks[c]]\n",
    "            f1s = []\n",
    "            best_f1 = 0\n",
    "            best_net_st = None\n",
    "            for seed in SEEDS:\n",
    "                net_st, best_val_f1 = utils.train_lp_balanced_class_loss(device, train_datasets[c], val_datasets[c], train_kwargs, seed, use_tqdm=False)\n",
    "                f1s.append(best_val_f1)\n",
    "                if best_val_f1 > best_f1:\n",
    "                    best_f1 = best_val_f1\n",
    "                    best_net_st = net_st\n",
    "                    best_net_st['f1'] = best_f1\n",
    "            \n",
    "            best_net_st['f1s'] = f1s \n",
    "            torch.save(best_net_st, st_path) # best classifier for this emotion\n",
    "        same_lang_f1s.append(best_val_f1)\n",
    "\n",
    "        ## make predictions on the OOD langs and save them to a folder\n",
    "        weight, bias = best_net_st['weight'].to(device), best_net_st['bias'].to(device)\n",
    "        for ood_lang in OOD_LANGS:\n",
    "            if c in lang_targets[ood_lang]:\n",
    "                scores_ = torch.tensor(pca.transform(lang_embeddings[ood_lang]), device=device) @ weight.T + bias\n",
    "                predictions_ = utils.get_predictions_bce(scores_).cpu().numpy()\n",
    "                lang_mtds[ood_lang][c] = predictions_\n",
    "        ## id predictions for track a\n",
    "        if dev_embeddings is not None:\n",
    "            scores_ = dev_embeddings @ weight.T + bias\n",
    "            predictions_ = utils.get_predictions_bce(scores_).cpu().numpy()\n",
    "            same_lang_mtd[c] = predictions_\n",
    "    # save mtd with the predictions\n",
    "    for ood_lang in OOD_LANGS:\n",
    "        lang_mtds[ood_lang].to_csv(predictions_path_c + f'pred_{ood_lang}.csv', index=False)\n",
    "    \n",
    "    if dev_embeddings is not None:\n",
    "        same_lang_mtd.to_csv(predictions_path_a + f'pred_{lang}.csv', index=False)\n",
    "\n",
    "    print('MACRO F1 id', np.mean(same_lang_f1s))\n",
    "    print(same_lang_f1s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 256\n",
    "# eng\n",
    "# MACRO F1 id 0.3857757935588125\n",
    "# [0.2090909090909091, 0.5446428571428572, 0.32432432432432434, 0.3818181818181818, 0.46900269541778977]\n",
    "# deu\n",
    "# MACRO F1 id 0.3292291311365098\n",
    "# [0.4929577464788733, 0.48135593220338985, 0.20202020202020204, 0.3253968253968254, 0.36893203883495146, 0.10471204188481675]\n",
    "# esp\n",
    "# MACRO F1 id 0.4033548805779876\n",
    "# [0.46829268292682924, 0.5357142857142857, 0.28395061728395055, 0.46586345381526106, 0.34408602150537637, 0.32222222222222224]\n",
    "# deu-esp-eng\n",
    "# MACRO F1 id 0.4597713424663154\n",
    "# [0.4501278772378517, 0.49612403100775193, 0.5828729281767956, 0.36494252873563215, 0.4072524407252441, 0.4573082489146165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 128\n",
    "# eng\n",
    "# MACRO F1 id 0.4322900239688668\n",
    "# [0.1610169491525424, 0.6736111111111112, 0.3882063882063882, 0.4396551724137931, 0.498960498960499]\n",
    "# deu\n",
    "# MACRO F1 id 0.3425377905415527\n",
    "# [0.5052631578947369, 0.5043859649122807, 0.2100456621004566, 0.32931726907630526, 0.37288135593220334, 0.13333333333333333]\n",
    "# esp\n",
    "# MACRO F1 id 0.40026781388339266\n",
    "# [0.4549763033175356, 0.49350649350649356, 0.2828282828282828, 0.47967479674796754, 0.3384615384615385, 0.3521594684385382]\n",
    "# deu-esp-eng\n",
    "# MACRO F1 id 0.45178584361714286\n",
    "# [0.4344473007712082, 0.4733420026007802, 0.5604395604395604, 0.3885167464114832, 0.4066390041493776, 0.4473304473304474]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 64\n",
    "# eng\n",
    "# MACRO F1 id 0.3540222874585907\n",
    "# [0.14225941422594143, 0.5217391304347826, 0.3286713286713286, 0.3630769230769231, 0.4143646408839779]\n",
    "# deu\n",
    "# MACRO F1 id 0.3161733738450248\n",
    "# [0.4697508896797153, 0.47213114754098356, 0.1935483870967742, 0.28461538461538466, 0.3545454545454545, 0.12244897959183672]\n",
    "# esp\n",
    "# MACRO F1 id 0.3475748573740769\n",
    "# [0.4158415841584158, 0.4524886877828054, 0.23030303030303031, 0.43859649122807015, 0.311377245508982, 0.23684210526315788]\n",
    "# deu-esp-eng\n",
    "# MACRO F1 id 0.44056359344184365\n",
    "# [0.4178343949044586, 0.45161290322580644, 0.5469613259668508, 0.3795811518324608, 0.4087193460490463, 0.43867243867243866]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
